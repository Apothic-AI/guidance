## Provider
DeepInfra (`/v1/openai/chat/completions`, OpenAI-compatible API).

## Grammar capability
- Documented constrained output is **JSON-mode / JSON-Schema structured output**, not general CFG/regex decoding.
- I did **not** find official DeepInfra docs exposing request fields for raw `grammar`, `cfg`, or `regex` constrained decoding.

## Request shape
- JSON mode:
  - `response_format: { "type": "json_object" }`
- Structured outputs (schema-constrained):
  - `response_format: { "type": "json_schema", "json_schema": { "name": "...", "schema": { ... }, "strict": true } }`

## Grammar dialect and caveats
- Dialect is **JSON Schema-style** constraints via `response_format.type = "json_schema"`.
- For `json_object`, DeepInfra docs say your messages must explicitly mention `"json"` (or request is rejected).
- No official DeepInfra documentation found for separate EBNF/CFG/regex grammar dialect parameters.

## Streaming fields
For streaming chat completions (`"stream": true`), relevant fields are standard OpenAI-style chunks:
- `object: "chat.completion.chunk"`
- `choices[].delta.role`
- `choices[].delta.content` (JSON arrives incrementally here)
- `choices[].finish_reason`
- terminal SSE line: `data: [DONE]`
- No separate ‚Äúconstraint-satisfied‚Äù streaming field is documented.

## Sources (URLs)
- https://deepinfra.com/docs/advanced/json_mode
- https://deepinfra.com/docs/openai_api
- https://deepinfra.com/deepseek-ai/DeepSeek-V3.1/api
- https://deepinfra.com/blog/json-mode

## stderr
```
OpenAI Codex v0.101.0 (research preview)
--------
workdir: /home/bitnom/Code/apothic-monorepo/libs/python/guidance
model: gpt-5.3-codex
provider: openai
approval: never
sandbox: danger-full-access
reasoning effort: low
reasoning summaries: auto
session id: 019c5b0c-fd87-7a13-a28a-bd0bb5eb39b5
--------
user
Research official documentation for provider "DeepInfra" focused on grammar-constrained generation.
Find whether they support grammar/CFG/regex constrained decoding, exact request fields, grammar dialect,
and streaming response fields relevant to constrained output.

Return concise markdown with sections:
- Provider
- Grammar capability
- Request shape
- Grammar dialect and caveats
- Streaming fields
- Sources (URLs)
mcp: playwright starting
mcp: surrealdb starting
mcp: exa starting
2026-02-14T07:28:23.977242Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5531-6c60-7612-b81c-b02333263b22
2026-02-14T07:28:24.005968Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5080-050b-7c53-92a8-9c52ce325359
2026-02-14T07:28:24.040914Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c58f2-9999-7e13-8897-cb3cb6765c62
2026-02-14T07:28:24.057123Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5ae2-4d18-7921-a6f9-9ca09a337133
2026-02-14T07:28:24.084468Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b06-1e63-7422-8df1-29cc3cd854ed
2026-02-14T07:28:24.122793Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b06-2af1-7321-8d40-367f5a3ba03a
2026-02-14T07:28:24.154173Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b06-8eca-7cf3-8c60-d66e26ea0144
2026-02-14T07:28:24.177496Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b07-1935-7772-bf75-7eb9d084599e
2026-02-14T07:28:24.193720Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b07-45eb-7810-9be4-e9c813827907
2026-02-14T07:28:24.217448Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b07-5630-7b30-8723-45187528edea
2026-02-14T07:28:24.259289Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b0a-f457-71d1-902a-d41b0df6fafa
2026-02-14T07:28:24.280681Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b0a-f500-7013-80bc-cc4e70470657
2026-02-14T07:28:24.316202Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b0c-1200-74e2-b68a-b5695adf8ddc
2026-02-14T07:28:24.334606Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b0c-4f6c-79a1-ad3e-23dd8a997ea2
2026-02-14T07:28:24.376123Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b0c-61e2-75e1-99f8-269a75536055
2026-02-14T07:28:24.415112Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b0c-61e0-7850-aab6-44bd39f1876c
2026-02-14T07:28:24.431179Z ERROR codex_core::rollout::list: state db missing rollout path for thread 019c5b0c-61e3-7490-b15b-7731fbbd70ff
mcp: surrealdb ready
mcp: playwright ready
mcp: exa ready
mcp startup: ready: surrealdb, playwright, exa

thinking
**Planning web search strategy**
üåê Searching the web...
üåê Searched: DeepInfra official documentation grammar constrained generation regex CFG

thinking
**Investigating structured output methods**
üåê Searching the web...
üåê Searched: site:deepinfra.com "guided_regex"

thinking
**Planning docs improvements**
üåê Searching the web...
üåê Searched the web

thinking
**Searching external docs for API info**
üåê Searching the web...
üåê Searched: site:deepinfra.com/docs OpenAI-compatible HTTP API DeepInfra chat completions parameters

thinking
**Choosing deeper doc source**
codex
I‚Äôm checking DeepInfra‚Äôs official API docs directly now to confirm constrained decoding support and exact streaming/request fields, then I‚Äôll summarize only what is explicitly documented.
tool exa.web_search_exa({"query":"DeepInfra official docs constrained decoding grammar regex CFG chat completions","numResults":5,"livecrawl":"preferred"})
exa.web_search_exa({"query":"DeepInfra official docs constrained decoding grammar regex CFG chat completions","numResults":5,"livecrawl":"preferred"}) success in 2.98s:
{
  "content": [
    {
      "type": "text",
      "text": "Title: Reliable JSON-Only Responses with DeepInfra LLMs\nAuthor: DeepInfra\nPublished Date: 2026-02-02T00:00:00.000Z\nURL: https://deepinfra.com/blog/deepinfra-json-only-responses\nText: Reliable JSON-Only Responses with DeepInfra LLMs\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \nReliable JSON-Only Responses with DeepInfra LLMs\nPublished on 2026.02.02 by DeepInfra\n[] [] [] \n![Reliable JSON-Only Responses with DeepInfra LLMs] \nWhen large language models are used inside real applications, their role changes fundamentally. Instead of chatting with users, they become infrastructure components: extracting information, transforming text, driving workflows, or powering APIs.\nIn these scenarios, natural language is no longer the desired output. What applications need is structured data ‚Äîand very often, that structure is JSON. Yet anyone who has worked with LLMs knows the problem: even when explicitly instructed, models frequently return explanations, markdown, or subtly invalid JSON. A single stray character is enough to break a production system.\nThis article explains how to reliably force DeepInfra-hosted LLMs to return valid JSON only, without fragile prompt hacks or complex post-processing. The approach is simple, robust, and well-suited for production environments.\nWe will walk through the reasoning, the design principles, and a complete Python example using DeepInfra‚Äôs OpenAI-compatible API.\n## **Why JSON-Only Output Matters in Practice**\nIn early experiments, developers often tolerate imperfect outputs. A human can easily ignore\n\nTitle: Flexible Grammar-Based Constrained Decoding for Language Models\nPublished Date: 2023-05-23T00:00:00.000Z\nURL: https://deepai.org/publication/flexible-grammar-based-constrained-decoding-for-language-models\nText: Flexible Grammar-Based Constrained Decoding for Language Models | DeepAI\n\n[DeepAI Pro - More Access to Better AI] √ó\n\n# Flexible Grammar-Based Constrained Decoding for Language Models\n\nLLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to reliably generating complex output structures, such as those required for information extraction. This limitation stems from the fact that LLMs, without fine-tuning, tend to generate free text rather than structures precisely following a specific grammar. In this work, we propose to enrich the decoding with formal grammar constraints. More concretely, given Context-Free Grammar(CFG), our framework ensures that the token generated in each decoding step would lead to a valid continuation compliant with the grammar production rules. This process guarantees the generation of valid sequences. Importantly, our framework can be readily combined with any CFG or decoding algorithm. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. We conducted experiments with two challenging tasks involving large alphabets in their grammar (Wikidata entities and relations): information extraction and entity disambiguation. Our results with LLaMA models indicate that grammar-constrained decoding substantially outperforms unconstrained decoding and even competes with task-specific fine-tuned models. These findings suggest that integrating grammar-based constraints during decoding holds great promise in making LLMs reliably produce structured outputs, especially in setting where training data is scarce and fine-tuning is expensive.\n\n[READ FULL TEXT] \n\n## Login\n\nPlease sign up or login with your\n\nTitle: Constrained Decoding: Grammar-Guided Generation for Structured ...\nAuthor: Michael Brenndoerfer\nPublished Date: 2025-07-29T00:00:00.000Z\nURL: https://mbrenndoerfer.com/writing/constrained-decoding-structured-llm-output\nText: Constrained Decoding: Grammar-Guided Generation for Structured LLM Output - Interactive | Michael Brenndoerfer | Michael Brenndoerfer\n[Writing] [Books] [About] [Community] [Contact] [Community] \nSign-in\n![]![] \nBack# Constrained Decoding: Grammar-Guided Generation for Structured LLM Output\nMichael BrenndoerferPublished:July 29, 2025Updated:July 31, 2025‚Ä¢UpdatedJuly 31, 2025‚Ä¢42min read\n[Data, Analytics &amp; AI] [Language AI Handbook] [Machine Learning] \nLearn how constrained decoding forces language models to generate valid JSON, SQL, and regex-matching text through token masking and grammar-guided generation.\n[\n![Language AI Handbook Cover] \nPart ofLanguage AI Handbook\nThis article is part of the free-to-readLanguage AI Handbook\n] \nReading Level\nChoose your expertise level to adjust how many terms are explained. Beginners see more tooltips, experts see fewer to maintain reading flow. Hover overunderlined termsfor instant definitions.\nBeginnerMaximum helpIntermediateMedium helpExpertMinimal helpHide AllNo tooltips\n## [] Constrained DecodingLink Copied\nLanguage models generate text bysamplingfrom aprobability distributionover thevocabularyat each step. This works well for creative writing, conversational responses, and open-ended generation. But what happens when you need the model to produce structured output, like validJSON, SQL queries, or text matching a specific pattern? Standardsamplingoffers no guarantees. The model might produce syntactically invalidJSON, hallucinate field names, or ignore your carefully specifiedschema.\nConstrained decodingsolves this problem by modifying the generation process itself. Instead of allowing the model to choose freely from all tokens at each step, we restrict it to tokens that keep the output on a \n\nTitle: [PDF] Constrained Decoding of Diffusion LLMs with Context-Free Grammars\nPublished Date: 2025-08-13T00:00:00.000Z\nURL: https://arxiv.org/pdf/2508.10111\nText: [We gratefully acknowledge support from\\\nthe Simons Foundation and member institutions.]\n\nTitle: llama.cpp/grammars/README.md at master ¬∑ ggml-org ... - GitHub\nURL: https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md\nText: llama.cpp/grammars/README.md at master ¬∑ggml-org/llama.cpp ¬∑GitHub\n[Skip to content] \n## Navigation Menu\nToggle navigation\n[] \n[Sign in] \nAppearance settings\nSearch or jump to...\n# Search code, repositories, users, issues, pull requests...\n \nSearch\nClear\n[Search syntax tips] \n# Provide feedback\n \nWe read every piece of feedback, and take your input very seriously.\nInclude my email address so I can be contacted\nCancelSubmit feedback\n# Saved searches\n## Use saved searches to filter your results more quickly\n \nName\nQuery\nTo see all available qualifiers, see our[documentation].\nCancelCreate saved search\n[Sign in] \n[Sign up] \nAppearance settings\nResetting focus\nYou signed in with another tab or window.[Reload] to refresh your session.You signed out in another tab or window.[Reload] to refresh your session.You switched accounts on another tab or window.[Reload] to refresh your session.Dismiss alert\n{{ message }}\n[ggml-org] /**[llama.cpp] **Public\n* [Notifications] You must be signed in to change notification settings\n* [Fork14.5k] \n* [Star93.1k] \n \nYou can‚Äôt perform that action at this time.\n\n\n"
    }
  ]
}
tool exa.web_search_exa({"query":"site:deepinfra.com \"Input Schema\" \"Streaming Schema\" \"chat.completion.chunk\"","numResults":5,"livecrawl":"preferred"})
exa.web_search_exa({"query":"site:deepinfra.com \"Input Schema\" \"Streaming Schema\" \"chat.completion.chunk\"","numResults":5,"livecrawl":"preferred"}) success in 1.27s:
{
  "content": [
    {
      "type": "text",
      "text": "Title: openchat/openchat_3.5 - API Reference - DeepInfra\nURL: https://deepinfra.com/openchat/openchat_3.5/api\nText: openchat/openchat\\_3.5 - API Reference - DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[Qwen3-Max-Thinking] state-of-the-art reasoning model at your fingertips!\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \nDue to low usage this model has been replaced by[meta-llama/Meta-Llama-3.1-8B-Instruct]. Your inference requests are still working but they are redirected. Please update your code to use another model.\nopenchat/\n# openchat\\_3.5\nOpenChat is a library of open-source language models that have been fine-tuned with C-RLFT, a strategy inspired by offline reinforcement learning. These models can learn from mixed-quality data without preference labels and have achieved exceptional performance comparable to ChatGPT. The developers of OpenChat are dedicated to creating a high-performance, commercially viable, open-source large language model and are continuously making progress towards this goal.\n[Deploy Private Endpoint] \nPublic\nfp16\n8,192\nJSON\n[Project] [Paper] [License] \n![openchat/openchat_3.5 cover image] \n[demo] api[versions] \nOpenAI Chat CompletionsAI SDKDeepInfra API\nHTTP/cURLPythonJavaScript\n### OpenAI-compatible HTTP API\nYou can POST to our OpenAI Chat Completions compatible endpoint.\n#### Simple messages and prompts\nGiven a list of messages from a conversation, the model will return a response.\n```\n`curl&quot;https://api.deepinfra.com/v1/openai/chat/completions&quot;\\\\\n-H&quot;Content-Type: application/json&quot;\\\\\n-H&quot;Authorization: Bearer$DEEPINFRA\\_TOKEN&quot;\\\\\n-d&#x27;&#x27;{\n&quot;&quot;model&\n\nTitle: Qwen/Qwen2.5-7B-Instruct - API Reference - DeepInfra\nAuthor: \nURL: https://deepinfra.com/Qwen/Qwen2.5-7B-Instruct/api\nText: Qwen/Qwen2.5-7B-Instruct - API Reference - DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \nDue to low usage this model has been replaced by[Qwen/Qwen3-14B]. Your inference requests are still working but they are redirected. Please update your code to use another model.\n![Qwen logo] \nQwen/\n# Qwen2.5-7B-Instruct\nThe 7 billion parameter Qwen2.5 excels in language understanding, multilingual capabilities, coding, mathematics, and reasoning\n[Deploy Private Endpoint] \nPublic\nbfloat16\n32,768\n[Project] [License] [Qwen] \n![Qwen/Qwen2.5-7B-Instruct cover image] \n[demo] api[versions] \nOpenAI Chat CompletionsAI SDKDeepInfra API\nHTTP/cURLPythonJavaScript\n### OpenAI-compatible HTTP API\nYou can POST to our OpenAI Chat Completions compatible endpoint.\n#### Simple messages and prompts\nGiven a list of messages from a conversation, the model will return a response.\n```\n`curl&quot;https://api.deepinfra.com/v1/openai/chat/completions&quot;\\\\\n-H&quot;Content-Type: application/json&quot;\\\\\n-H&quot;Authorization: Bearer$DEEPINFRA\\_TOKEN&quot;\\\\\n-d&#x27;{\n&quot;model&quot;: &quot;Qwen/Qwen2.5-7B-Instruct&quot;,\n&quot;messages&quot;: [\n{\n&quot;role&quot;: &quot;user&quot;,\n&quot;content&quot;: &quot;Hello!&quot;\n}\n]\n}&#x27;`\ncopy\n```\nTo which you&#x27;d get something like:\n```\n`{&quot;id&quot;:&quot;chatcmpl-guMTxWgpFf&quot;,&quot;object&quot;:&quot;chat.completion&quot;,&quot;created&quot;:1694623155,&quot;model&\n\nTitle: mistralai/Mistral-7B-Instruct-v0.2 - API Reference - DeepInfra\nURL: https://deepinfra.com/mistralai/Mistral-7B-Instruct-v0.2/api\nText: mistralai/Mistral-7B-Instruct-v0.2 - API Reference - DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \nDue to low usage this model has been replaced by[mistralai/Mistral-Small-3.2-24B-Instruct-2506]. Your inference requests are still working but they are redirected. Please update your code to use another model.\n![mistralai logo] \nmistralai/\n# Mistral-7B-Instruct-v0.2\nThe Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is a instruct fine-tuned version of the Mistral-7B-v0.2 generative text model using a variety of publicly available conversation datasets.\n[Deploy Private Endpoint] \nPublic\nfp16\n32,768\n![mistralai/Mistral-7B-Instruct-v0.2 cover image] \n[demo] api[versions] \nOpenAI Chat CompletionsAI SDKDeepInfra API\nHTTP/cURLPythonJavaScript\n### OpenAI-compatible HTTP API\nYou can POST to our OpenAI Chat Completions compatible endpoint.\n#### Simple messages and prompts\nGiven a list of messages from a conversation, the model will return a response.\n```\n`curl&quot;https://api.deepinfra.com/v1/openai/chat/completions&quot;\\\\\n-H&quot;Content-Type: application/json&quot;\\\\\n-H&quot;Authorization: Bearer$DEEPINFRA\\_TOKEN&quot;\\\\\n-d&#x27;{\n&quot;model&quot;: &quot;mistralai/Mistral-7B-Instruct-v0.2&quot;,\n&quot;messages&quot;: [\n{\n&quot;role&quot;: &quot;user&quot;,\n&quot;content&quot;: &quot;Hello!&quot;\n}\n]\n}&#x27;`\ncopy\n```\nTo which you&#x27;d get something like:\n```\n`{&quot;id&quot;:\n\nTitle: deepseek-ai/DeepSeek-V3.1 - API Reference - DeepInfra\nURL: https://deepinfra.com/deepseek-ai/DeepSeek-V3.1/api\nText: deepseek-ai/DeepSeek-V3.1 - API Reference - DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n![deepseek-ai logo] \ndeepseek-ai/\n# DeepSeek-V3.1\n$0.21\nin\n$0.79\nout\n$0.13\ncached\n/ 1M tokens\nDeepSeek-V3.1 is post-trained on the top of DeepSeek-V3.1-Base, which is built upon the original V3 base checkpoint through a two-phase long context extension approach, following the methodology outlined in the original DeepSeek-V3 report. We have expanded our dataset by collecting additional long documents and substantially extending both training phases. The 32K extension phase has been increased 10-fold to 630B tokens, while the 128K extension phase has been extended by 3.3x to 209B tokens. Additionally, DeepSeek-V3.1 is trained using the UE8M0 FP8 scale data format to ensure compatibility with microscaling data formats.\n[Deploy Private Endpoint] \nPublic\nfp4\n163,840\nJSON\nFunction\n[Project] [Paper] [License] [DeepSeek] \n![deepseek-ai/DeepSeek-V3.1 cover image] \n[demo] api[versions] \nOpenAI Chat CompletionsAI SDKDeepInfra API\nHTTP/cURLPythonJavaScript\n### OpenAI-compatible HTTP API\nYou can POST to our OpenAI Chat Completions compatible endpoint.\n#### Simple messages and prompts\nGiven a list of messages from a conversation, the model will return a response.\n```\n`curl&quot;https://api.deepinfra.com/v1/openai/chat/completions&quot;\\\\\n-H&quot;Content-Type: application/json&quot;\\\\\n-H&quot;Authorization: Bearer$DEEPI\n\nTitle: deepseek-ai/DeepSeek-V3 - API Reference - DeepInfra\nAuthor: \nPublished Date: 2025-01-03T00:00:00.000Z\nURL: https://deepinfra.com/deepseek-ai/DeepSeek-V3/api\nText: deepseek-ai/DeepSeek-V3 - API Reference - DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n![deepseek-ai logo] \ndeepseek-ai/\n# DeepSeek-V3\n$0.32\nin\n$0.89\nout\n/ 1M tokens\nDeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2.\n[Deploy Private Endpoint] \nPublic\nfp4\n163,840\nJSON\nFunction\n[Project] [License] [DeepSeek] \n![deepseek-ai/DeepSeek-V3 cover image] \n[demo] api[versions] \nOpenAI Chat CompletionsAI SDKDeepInfra API\nHTTP/cURLPythonJavaScript\n### OpenAI-compatible HTTP API\nYou can POST to our OpenAI Chat Completions compatible endpoint.\n#### Simple messages and prompts\nGiven a list of messages from a conversation, the model will return a response.\n```\n`curl&quot;https://api.deepinfra.com/v1/openai/chat/completions&quot;\\\\\n-H&quot;Content-Type: application/json&quot;\\\\\n-H&quot;Authorization: Bearer$DEEPINFRA\\_TOKEN&quot;\\\\\n-d&#x27;{\n&quot;model&quot;: &quot;deepseek-ai/DeepSeek-V3&quot;,\n&quot;messages&quot;: [\n{\n&quot;role&quot;: &quot;user&quot;,\n&quot;content&quot;: &quot;Hello!&quot;\n}\n]\n}&#x27;`\ncopy\n```\nTo which you&#x27;d get something like:\n```\n`{&quot;id&quot;:&quot;chatcmpl-guMTxWgpFf&\n\n"
    }
  ]
}

thinking
**Checking API input schema**
üåê Searching the web...
üåê Searched: https://deepinfra.com/deepseek-ai/DeepSeek-V3.1/api

thinking
**Planning schema discovery approach**
üåê Searching the web...
üåê Searched: 'guided' in https://deepinfra.com/deepseek-ai/DeepSeek-V3.1/api

thinking
**Exploring schema retrieval options**
codex
I found the model API page and verified the streaming chunk structure, but I still need the exact constrained-decoding request parameters. I‚Äôm now checking DeepInfra‚Äôs docs/blog pages for the explicit fields and dialect details.
üåê Searching the web...
üåê Searched: site:deepinfra.com "response_format" "DeepInfra"
tool exa.web_search_exa({"query":"DeepInfra OpenAI compatible structured output response_format json_schema strict","numResults":8,"livecrawl":"preferred"})
exa.web_search_exa({"query":"DeepInfra OpenAI compatible structured output response_format json_schema strict","numResults":8,"livecrawl":"preferred"}) success in 1.56s:
{
  "content": [
    {
      "type": "text",
      "text": "Title: Reliable JSON-Only Responses with DeepInfra LLMs\nAuthor: DeepInfra\nPublished Date: 2026-02-02T00:00:00.000Z\nURL: https://deepinfra.com/blog/deepinfra-json-only-responses\nText: Reliable JSON-Only Responses with DeepInfra LLMs\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[Qwen3-Max-Thinking] state-of-the-art reasoning model at your fingertips!\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \nReliable JSON-Only Responses with DeepInfra LLMs\nPublished on 2026.02.02 by DeepInfra\n[] [] [] \n![Reliable JSON-Only Responses with DeepInfra LLMs] \nWhen large language models are used inside real applications, their role changes fundamentally. Instead of chatting with users, they become infrastructure components: extracting information, transforming text, driving workflows, or powering APIs.\nIn these scenarios, natural language is no longer the desired output. What applications need is structured data ‚Äîa\n\nTitle: Use JSON response format with DeepInfra endpoints | ML Models\nURL: https://deepinfra.com/docs/advanced/json_mode\nText: Use JSON response format with DeepInfra endpoints | ML Models | DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. [Running Inference] \n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. JSON Mode\n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Image Adapters] \n4. [Integrations] \n1. [L\n\nTitle: Introducing Structured Outputs - OpenAI Developer Community\nPublished Date: 2024-08-06T00:00:00.000Z\nURL: https://community.openai.com/t/introducing-structured-outputs/896022\nText: Introducing Structured Outputs - Announcements - OpenAI Developer Community\n[Skip to last reply] [Skip to top] \n[Skip to main content] \n[![OpenAI Developer Community]] \n* [Docs] \n* [API] \n* [Support] \nSign UpLog In\n* ‚Äã* [Topics] \n* More\nResources\n* [Documentation] \n* [API reference] \n* [Help center] \nCategories\n* [Announcements] \n* [API] \n* [Prompting] \n* [Documentation] \n* [Plugins / Actions builders] \n* [All categories] \nTags\n* [chatgpt] \n* [gpt-4] \n* [lost-user] \n* [api] \n* [assistants-api] \n* [All tags] \nLight mode\n‚Äã‚ÄãWelcome to the OpenAI Developer Community, a forum for**developers to meet and chat with other developers**while building with OpenAI‚Äôs APIs and developer platform.\nThis is not a place for ChatGPT discussion (with the exception of ChatGPT developer tools, like Codex). ChatGPT discussion takes place in the[OpenAI Discord community].\nWhat to know:\n1. This forum is community-run and maintained. Not all posts are monitored. To get in touch with OpenAI, reach out at[https://help.openai.com/en/?q=contact].\n2. Discussions related to t\n\nTitle: Guaranteed JSON output on Open-Source LLMs.\nAuthor: \nPublished Date: 2024-03-08T00:00:00.000Z\nURL: https://deepinfra.com/blog/json-mode\nText: We use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\n\nAcceptReject\n\n[FLUX.2 is live!] High-fidelity image generation made simple.\n\n[Models] \n\n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n\n[Docs] \n\n[Pricing] \n\n[GPUs] \n\n[Chat] \n\n[DeepStart] \n\n[Blog] \n\nFeedback\n\n[Contact Sales] \n\n[Log In] \n\nGuaranteed JSON output on Open-Source LLMs.\n\nPublished on 2024.03.08 by Patrick Reiter Horn\n\nDeepInfra is proud to announce that we have released \"JSON mode\" across _all_ of our text language models. It is available through the `\"response_format\"` object, which currently supports only `{\"type\": \"json_object\"}`\n\nOur JSON mode will guarantee that all tokens returned in the output of a language model completion or chat response conforms to valid [JSON (JavaScript Object Notation)].\n\nThe JSON format carries no performance overhead, and the feature is already available on all of our models\n\nTitle: Structured model outputs | OpenAI API\nURL: https://developers.openai.com/api/docs/guides/structured-outputs/\nText: # Structured model outputs\n\nJSON is one of the most widely used formats in the world for applications to exchange data.\n\nStructured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied [JSON Schema], so you don't need to worry about the model omitting a required key, or hallucinating an invalid enum value.\n\nSome benefits of Structured Outputs include:\n\n1. **Reliable type-safety:** No need to validate or retry incorrectly formatted responses\n1. **Explicit refusals:** Safety-based model refusals are now programmatically detectable\n1. **Simpler prompting:** No need for strongly worded prompts to achieve consistent formatting\n\nIn addition to supporting JSON Schema in the REST API, the OpenAI SDKs for [Python] and [JavaScript] also make it easy to define object schemas using [Pydantic] and [Zod] respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.\n\n### Supported models\n\nStructured Outputs is available in our [latest large language models], starting with GPT-4o. Older models like `gpt-4-turbo`\n\nTitle: Structured model outputs\nAuthor: \nPublished Date: 2024-08-06T00:00:00.000Z\nURL: https://platform.openai.com/docs/guides/structured-outputs#supported-schemas\nText: # Structured model outputs\n\nJSON is one of the most widely used formats in the world for applications to exchange data.\n\nStructured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied [JSON Schema], so you don't need to worry about the model omitting a required key, or hallucinating an invalid enum value.\n\nSome benefits of Structured Outputs include:\n\n1. **Reliable type-safety:** No need to validate or retry incorrectly formatted responses\n1. **Explicit refusals:** Safety-based model refusals are now programmatically detectable\n1. **Simpler prompting:** No need for strongly worded prompts to achieve consistent formatting\n\nIn addition to supporting JSON Schema in the REST API, the OpenAI SDKs for [Python] and [JavaScript] also make it easy to define object schemas using [Pydantic] and [Zod] respectively. Below, you can see how to extract information from unstructured text that conforms to a schema defined in code.\n\n### Supported models\n\nStructured Outputs is available in our [latest large language models], start\n\nTitle: OpenAI Structured Outputs: Complete Developer Guide\nAuthor: Digital Applied\nPublished Date: 2026-01-12T00:00:00.000Z\nURL: https://www.digitalapplied.com/blog/openai-structured-outputs-complete-guide\nText: OpenAI Structured Outputs: Complete Developer Guide\nAI Development10min read\n# OpenAI Structured Outputs: Complete Developer Guide\nMaster OpenAI Structured Outputs: JSON mode, function calling, and response formatting. Reliable data extraction with guaranteed schema compliance.\nDigital Applied Team\nJanuary 13, 2026\n10min read\n&gt;92%\nGPT-5.2 Reliability\n16k\nOutput Token Limit\n100%\nSchema Compliance\n30%\nHallucination Reduction\n## Key Takeaways\n100% Schema Compliance via CFG Engine:GPT-5.2 uses a Context-Free Grammar engine to mask invalid tokens before generation - it literally cannot produce a non-conforming response\nStrict Mode is the New Default:JSON Mode (type: json\\_object) only guarantees valid JSON syntax and is now &#x27;&#x27;legacy&#x27;&#x27;. Strict mode guarantees full schema adherence for production\nSchema-First Development:The paradigm shift: define schemas in Zod (TypeScript) or Pydantic (Python) first, then build prompts around them. This is now standard practice\nHandle Refusals as First-Class Errors:Stru\n\nTitle: json - OpenAI API error: \"Invalid parameter: 'response_format' of type ...\nPublished Date: 2024-09-30T00:00:00.000Z\nURL: https://stackoverflow.com/questions/79039544/openai-api-error-invalid-parameter-response-format-of-type-json-schema-is\nText: json - OpenAI API error: \"Invalid parameter: 'response\\_format' of type 'json\\_schema' is not supported with model version `gpt-4-turbo`\" - Stack Overflow\n# ![site logo] ****\nBy clicking ‚ÄúSign up‚Äù, you agree to our[terms of service] and acknowledge you have read our[privacy policy].\nSign up with Google\nSign up with GitHub\n# OR\nEmail\nPassword\nSign up\nAlready have an account?[Log in] \n[Skip to main content] \n[] \n[Stack Overflow] \n1. [About] \n2. Products\n3. [For Teams] \n1. [Stack InternalImplement a knowledge platform layer to power your enterprise and AI tools.] \n2. [Stack Data LicensingGet access to top-class technical expertise with trusted &amp; attributed content.] \n3. [Stack AdsConnect your brand to the world‚Äôs most trusted technologist communities.] \n4. [ReleasesKeep up-to-date on features we add to Stack Overflow and Stack Internal.] \n5. [About the company] [Visit the blog] \nLoading‚Ä¶\n# Let's set up your homepageSelect a few topics you're interested in:\npythonjavascriptc#react\n\n"
    }
  ]
}
tool exa.web_search_exa({"query":"site:deepinfra.com/blog deepinfra json only responses python example response_format","numResults":5,"livecrawl":"preferred"})
exa.web_search_exa({"query":"site:deepinfra.com/blog deepinfra json only responses python example response_format","numResults":5,"livecrawl":"preferred"}) success in 1.31s:
{
  "content": [
    {
      "type": "text",
      "text": "Title: Reliable JSON-Only Responses with DeepInfra LLMs\nAuthor: DeepInfra\nPublished Date: 2026-02-02T00:00:00.000Z\nURL: https://deepinfra.com/blog/deepinfra-json-only-responses\nText: Reliable JSON-Only Responses with DeepInfra LLMs\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[Qwen3-Max-Thinking] state-of-the-art reasoning model at your fingertips!\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \nReliable JSON-Only Responses with DeepInfra LLMs\nPublished on 2026.02.02 by DeepInfra\n[] [] [] \n![Reliable JSON-Only Responses with DeepInfra LLMs] \nWhen large language models are used inside real applications, their role changes fundamentally. Instead of chatting with users, they become infrastructure components: extracting information, transforming text, driving workflows, or powering APIs.\nIn these scenarios, natural language is no longer the desired output. What applications need is structured data ‚Äîand very often, that structure is JSON. Yet anyone who has worked with LLMs knows the problem: even when explicitly instructed, models frequently return explanations, markdown, or subtly invalid JSON. A single stray character is enough to break a production system.\nThis article explains how to reliably force DeepInfra-hosted LLMs to return valid JSON only, without fragile prompt hacks or complex post-processing. The approach is simple, robust, and well-suited for production environments.\nWe will walk through the reasoning, the design principles, and a complete Python example using DeepInfra‚Äôs OpenAI-compatible API.\n## **Why JSON-Only Output Matters in Practice**\nIn early experiments, developers often tolerate imperfect outputs. A human can e\n\nTitle: Use JSON response format with DeepInfra endpoints | ML Models | DeepInfra\nAuthor: \nURL: https://deepinfra.com/docs/advanced/json_mode\nText: Use JSON response format with DeepInfra endpoints | ML Models | DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. [Running Inference] \n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. JSON Mode\n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Image Adapters] \n4. [Integrations] \n1. [LangChain] \n2. [LlamaIndex] \n3. [AI SDK] \n4. [AutoGen] \n5. [Okta SSO] \n6. [Tutorials &amp; Examples] \n1. [Stable Diffusion (Text to Image)] \n2. [Whisper (Speech to Text)] \n3. [Deprecated Models] \n4. [Miscellaneous] \n1. [Data Subprocessors] \nDocumentation\n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. [Running Inference] \n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. JSON Mode\n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs\n\nTitle: Use OpenAI API clients with LLaMas | ML Models - DeepInfra\nURL: https://deepinfra.com/docs/openai_api\nText: Use OpenAI API clients with LLaMas | ML Models | DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. [Running Inference] \n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. OpenAI-Compatible API\n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. [JSON Mode] \n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Image Adapters] \n4. [Integrations] \n1. [LangChain] \n2. [LlamaIndex] \n3. [AI SDK] \n4. [AutoGen] \n5. [Okta SSO] \n6. [Tutorials &amp; Examples] \n1. [Stable Diffusion (Text to Image)] \n2. [Whisper (Speech to Text)] \n3. [Deprecated Models] \n4. [Miscellaneous] \n1. [Data Subprocessors] \nDocumentation\n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. [Running Inference] \n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. OpenAI-Compatible API\n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. [JSON Mode] \n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Image Ada\n\nTitle: Guaranteed JSON output on Open-Source LLMs.\nAuthor: \nURL: https://deepinfra.com/blog/json-mode\nText: We use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\n\nAcceptReject\n\n[FLUX.2 is live!] High-fidelity image generation made simple.\n\n[Models] \n\n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n\n[Docs] \n\n[Pricing] \n\n[GPUs] \n\n[Chat] \n\n[DeepStart] \n\n[Blog] \n\nFeedback\n\n[Contact Sales] \n\n[Log In] \n\nGuaranteed JSON output on Open-Source LLMs.\n\nPublished on 2024.03.08 by Patrick Reiter Horn\n\nDeepInfra is proud to announce that we have released \"JSON mode\" across _all_ of our text language models. It is available through the `\"response_format\"` object, which currently supports only `{\"type\": \"json_object\"}`\n\nOur JSON mode will guarantee that all tokens returned in the output of a language model completion or chat response conforms to valid [JSON (JavaScript Object Notation)].\n\nThe JSON format carries no performance overhead, and the feature is already available on all of our models for free. Please try it out!\n\n## Using JSON mode\n\nActivating a JSON response in any of deepinfra's text APIs, including `/v1/inference`, `/v1/openai/completions` and `/v1/openai/chat/completions` is performed in the same way: adding a parameter `response_format` and setting its value to `{\"type\": \"json_object\"}`\n\nFor the best quality responses, it is also recommended to prompt the model to produce JSON, perhaps also indicating which fields to include in the resulting object.\n\n### Example of JSON mode\n\nHere is an example of using the openai chat API to invoke a model with JSON mode:\n\n```\nmessages = [\n {\n \"role\": \"user\",\n \"content\": \"Provide a JSON list of 3 famous scientific breakthroughs in the past century, all of the countries which contributed, and in what year.\"\n }\n]\n\nrespons\n\nTitle: AI Inference Platform | Deep Infra\nURL: https://deepinfra.com/docs/inference\nText: Inference | AI Inference Platform | Deep Infra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. Running Inference\n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. [JSON Mode] \n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Image Adapters] \n4. [Integrations] \n1. [LangChain] \n2. [LlamaIndex] \n3. [AI SDK] \n4. [AutoGen] \n5. [Okta SSO] \n6. [Tutorials &amp; Examples] \n1. [Stable Diffusion (Text to Image)] \n2. [Whisper (Speech to Text)] \n3. [Deprecated Models] \n4. [Miscellaneous] \n1. [Data Subprocessors] \nDocumentation\n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. Running Inference\n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. [JSON Mode] \n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Image Adapters] \n4. [Integrations] \n1. [LangCh\n\n"
    }
  ]
}








thinking
**Verifying exact response fields**
thinking
**Identifying JSON schema caveats**
thinking
**Planning JSON keyword detection**
thinking
**Identifying streaming fields in API schema**
thinking
**Locating streaming API examples**
thinking
**Finding OpenAI API stream example**
thinking
**Considering scroll lines for stream**
thinking
**Verifying regex support from docs**
codex
I‚Äôve confirmed DeepInfra‚Äôs documented constrained output path (`response_format`) and now I‚Äôm doing one last pass for any official `regex`/`CFG`/`grammar` decoding fields so we can distinguish supported vs undocumented.
tool exa.web_search_exa({"query":"site:deepinfra.com/docs DeepInfra regex grammar cfg constrained decoding","numResults":8,"livecrawl":"preferred"})
exa.web_search_exa({"query":"site:deepinfra.com/docs DeepInfra regex grammar cfg constrained decoding","numResults":8,"livecrawl":"preferred"}) success in 1.16s:
{
  "content": [
    {
      "type": "text",
      "text": "Title: Advanced topics | AI Inference Platform | Deep Infra\nAuthor: \nURL: https://deepinfra.com/docs/advanced\nText: Advanced topics | AI Inference Platform | Deep Infra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[Qwen3-Max-Thinking] state-of-the-art reasoning model at your fingertips!\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n# Advanced topics\nDeepInfra also supports advanced use cases which are covered in the following topics.\n* [Rate Limits] \n* [Webhooks] \n* [Custom LLMs] \n* [Log Probabilities] \n* [Function Calling] \n* [JSON mode] \n* [Multimodal models] \n* [AutoGen] \n* [LangChain] \n* [LLamaIndex] \n* [Deprecated models] \n![Footer Logo] \n![SOC 2 Certified]![ISO 27001 Certified] \nHave questions or need a custom solution?\n[Contact Sales] \nCompany\n[Pricing] \n[Docs] \n[Compare] \n[DeepStart] \n[About] \n[Careers] \n[Contact us] \n[Trust Center] \n[DeepGPT] \nLatest Models\n[deepseek-ai/DeepSeek-V3.2-E\n\nTitle: Returning Log probabilities | ML Models | DeepInfra\nAuthor: \nPublished Date: 2025-01-01T00:00:00.000Z\nURL: https://deepinfra.com/docs/advanced/log_probs\nText: Returning Log probabilities | ML Models | DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[Qwen3-Max-Thinking] state-of-the-art reasoning model at your fingertips!\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. [Running Inference] \n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. [JSON Mode] \n3. [Multimodal Models] \n4. Log Probabilities\n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Image Adapt\n\nTitle: Use AutoGen with DeepInfra endpoints | ML Models | DeepInfra\nAuthor: \nPublished Date: 2025-01-01T00:00:00.000Z\nURL: https://deepinfra.com/docs/advanced/autogen\nText: Use AutoGen with DeepInfra endpoints | ML Models | DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. [Running Inference] \n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. [JSON Mode] \n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Imag\n\nTitle: Use JSON response format with DeepInfra endpoints | ML Models | DeepInfra\nAuthor: \nURL: https://deepinfra.com/docs/advanced/json_mode\nText: Use JSON response format with DeepInfra endpoints | ML Models | DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. [Running Inference] \n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. JSON Mode\n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Image Adapters] \n4. \n\nTitle: Understanding Max Output Tokens Limit of DeepInfra | ML Models | DeepInfra\nAuthor: \nPublished Date: 2025-01-01T00:00:00.000Z\nURL: https://deepinfra.com/docs/advanced/max_tokens_limit\nText: Understanding Max Output Tokens Limit of DeepInfra | ML Models | DeepInfra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. [Running Inference] \n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. [JSON Mode] \n3. [Multimodal Models] \n4. [Log Probabilities] \n5. Max Output Tokens\n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [L\n\nTitle: Custom Deployments | AI Inference Platform | Deep Infra\nAuthor: \nPublished Date: 2025-01-01T00:00:00.000Z\nURL: https://deepinfra.com/docs/custom-deployments\nText: Custom Deployments | AI Inference Platform | Deep Infra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[Qwen3-Max-Thinking] state-of-the-art reasoning model at your fingertips!\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. [Running Inference] \n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. [JSON Mode] \n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. Custom Deployments\n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Ima\n\nTitle: Models | AI Inference Platform | Deep Infra\nAuthor: \nPublished Date: 2025-01-01T00:00:00.000Z\nURL: https://deepinfra.com/docs/models\nText: Models | AI Inference Platform | Deep Infra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[Qwen3-Max-Thinking] state-of-the-art reasoning model at your fingertips!\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n1. [Getting Started] \n1. [Quick Start Guide] \n2. Available Models\n3. [Running Inference] \n4. [Data Privacy &amp; Security] \n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. [JSON Mode] \n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Image Adapters] \n4. [Integrations] \n1. \n\nTitle: Data privacy during Inference | AI Inference Platform | Deep Infra\nAuthor: \nPublished Date: 2025-01-01T00:00:00.000Z\nURL: https://deepinfra.com/docs/data\nText: Data privacy during Inference | AI Inference Platform | Deep Infra\nWe use essential cookies to make our site work. With your consent, we may also use non-essential cookies to improve user experience and analyze website traffic‚Ä¶\nAcceptReject\n[FLUX.2 is live!] High-fidelity image generation made simple.\n[Models] \n[Automatic Speech Recognition] [Embeddings] [Reranker] [Text Generation] [Text To Image] [Text To Speech] [Text To Video] [Zero Shot Image Classification] \n[Docs] \n[Pricing] \n[GPUs] \n[Chat] \n[DeepStart] \n[Blog] \nFeedback\n[Contact Sales] \n[Log In] \n1. [Getting Started] \n1. [Quick Start Guide] \n2. [Available Models] \n3. [Running Inference] \n4. Data Privacy &amp; Security\n5. [API Reference] \n1. [OpenAI-Compatible API] \n2. [DeepInfra Native API] \n3. [Rate Limits] \n4. [Webhooks] \n5. [Authentication &amp; Tokens] \n6. [Model Features] \n1. [Function Calling] \n2. [JSON Mode] \n3. [Multimodal Models] \n4. [Log Probabilities] \n5. [Max Output Tokens] \n6. [GPU Instances] \n1. [Containers] \n2. [Custom Deployments] \n1. [Custom LLMs] \n2. [LoRA Adapter Models] \n3. [LoRA Image A\n\n"
    }
  ]
}



thinking
**Checking OpenAI API docs**
thinking
**Verifying completion marker pattern**
thinking
**Summarizing JSON Mode Constraints**
codex
## Provider
DeepInfra (`/v1/openai/chat/completions`, OpenAI-compatible API).

## Grammar capability
- Documented constrained output is **JSON-mode / JSON-Schema structured output**, not general CFG/regex decoding.
- I did **not** find official DeepInfra docs exposing request fields for raw `grammar`, `cfg`, or `regex` constrained decoding.

## Request shape
- JSON mode:
  - `response_format: { "type": "json_object" }`
- Structured outputs (schema-constrained):
  - `response_format: { "type": "json_schema", "json_schema": { "name": "...", "schema": { ... }, "strict": true } }`

## Grammar dialect and caveats
- Dialect is **JSON Schema-style** constraints via `response_format.type = "json_schema"`.
- For `json_object`, DeepInfra docs say your messages must explicitly mention `"json"` (or request is rejected).
- No official DeepInfra documentation found for separate EBNF/CFG/regex grammar dialect parameters.

## Streaming fields
For streaming chat completions (`"stream": true`), relevant fields are standard OpenAI-style chunks:
- `object: "chat.completion.chunk"`
- `choices[].delta.role`
- `choices[].delta.content` (JSON arrives incrementally here)
- `choices[].finish_reason`
- terminal SSE line: `data: [DONE]`
- No separate ‚Äúconstraint-satisfied‚Äù streaming field is documented.

## Sources (URLs)
- https://deepinfra.com/docs/advanced/json_mode
- https://deepinfra.com/docs/openai_api
- https://deepinfra.com/deepseek-ai/DeepSeek-V3.1/api
- https://deepinfra.com/blog/json-mode
tokens used
32,883
```

